// /api/chat.js
import { addToVectorDB, queryVectorDB } from "../lib/vectorDB.js";

export default async function handler(req, res) {
  if (req.method !== "POST") return res.status(405).json({ text: "MÃ©thode non autorisÃ©e" });
  
  const { message } = req.body;
  if (!message) return res.status(400).json({ text: "Message vide" });

  const HF_TOKEN = process.env.HUGGINGFACE_API_KEY;

  try {
    // ðŸ”¹ 1. On peut crÃ©er l'embedding depuis HuggingFace
    const embeddingResponse = await fetch("https://api-inference.huggingface.co/embeddings/meta-llama/llama-text-embed-v2", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${HF_TOKEN}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({ inputs: message }),
    });
    const embeddingData = await embeddingResponse.json();
    const embedding = embeddingData[0]?.embedding;
    
    // ðŸ”¹ 2. Rechercher les vecteurs proches dans Pinecone
    const context = embedding ? await queryVectorDB(embedding, 3) : [];

    // ðŸ”¹ 3. PrÃ©parer le prompt avec contexte
    const promptWithContext = `
Voici des informations utiles tirÃ©es de la mÃ©moire de l'IA :
${context.join("\n")}
Utilisateur : ${message}
RÃ©ponds de maniÃ¨re claire et prÃ©cise :
`;

    // ðŸ”¹ 4. Envoyer Ã  HuggingFace pour la rÃ©ponse finale
    const response = await fetch("https://router.huggingface.co/v1/chat/completions", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${HF_TOKEN}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        model: "meta-llama/Meta-Llama-3-8B-Instruct",
        messages: [{ role: "user", content: promptWithContext }],
        temperature: 0.7,
        max_new_tokens: 512,
      }),
    });

    const data = await response.json();
    if (!response.ok) return res.status(500).json({ text: `Erreur IA provider : ${JSON.stringify(data)}` });

    const text = data?.choices?.[0]?.message?.content?.trim() || "ðŸ¤– Pas de rÃ©ponse du modÃ¨le.";

    // ðŸ”¹ 5. Ajouter la nouvelle question + rÃ©ponse Ã  Pinecone
    if (embedding) await addToVectorDB(`msg-${Date.now()}`, message + " | " + text, embedding);

    return res.status(200).json({ text });
  } catch (err) {
    return res.status(500).json({ text: `Erreur serveur : ${err.message}` });
  }
}
